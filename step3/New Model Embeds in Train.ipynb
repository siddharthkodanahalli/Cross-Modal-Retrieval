{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697bf182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from transformers import BertTokenizer, BertModel, ViTFeatureExtractor, ViTModel\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Resize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4747768",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc77ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd03809",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "image_feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "# image_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1143d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aee9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self,device, d_model, concat_size, num_classes, nlayers, nhead, d_hid, dropout=0.5):\n",
    "        \n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        \n",
    "        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(self.device)\n",
    "        \n",
    "        self.vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\").to(self.device)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.class_token = nn.Parameter(torch.rand(1, d_model))\n",
    "        \n",
    "        self.linear_layer = nn.Sequential(nn.Linear(concat_size,1), nn.Sigmoid())\n",
    "    \n",
    "    def preprocess(self,image, encoded_text_input):\n",
    "        \n",
    "        #input_img = self.vit_feature_extractor(image, return_tensors='pt').to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_embeds = self.bert_model.embeddings(input_ids = encoded_text_input['input_ids'], \n",
    "                                            token_type_ids = encoded_text_input['token_type_ids']).to(device)\n",
    "            image_embeds = self.vit_model.embeddings(image['pixel_values']).to(device)\n",
    "\n",
    "        return image_embeds, text_embeds\n",
    "        \n",
    "    def forward(self, images, texts):\n",
    "        image_patch_embeds, text_embeddings = self.preprocess(images, texts)\n",
    "        \n",
    "        concat_embeds = torch.cat([image_patch_embeds, text_embeddings], 1)\n",
    "        concat_embeds = torch.stack([torch.vstack((self.class_token, concat_embeds[i])) for i in range(len(concat_embeds))])\n",
    "        pos_embed = get_positional_embeddings(concat_embeds.shape[1], self.d_model).repeat(concat_embeds.shape[0], 1, 1)\n",
    "        concat_embeds+= pos_embed.to(self.device)\n",
    "        \n",
    "        logits = self.transformer_encoder(concat_embeds)\n",
    "        logits = logits[:,0,:]\n",
    "        \n",
    "        preds = self.linear_layer(logits)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e2e0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, annotations, img_dir):\n",
    "        self.labels = pd.read_csv(annotations)\n",
    "        self.img_dir = img_dir \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels.iloc[idx,1]\n",
    "        img_path = self.img_dir + img_name[0] + \"/\" + \\\n",
    "                img_name[1] + \"/\" + img_name[2] + \"/\" + img_name[3] + \"/\" + img_name\n",
    "        \n",
    "        im = read_image(img_path)\n",
    "        \n",
    "        recipe = self.labels.iloc[idx,3]\n",
    "        \n",
    "        label = self.labels.iloc[idx, 4]\n",
    "        \n",
    "        #t_embeds, img_embeds = self.preprocess(im, recipe)\n",
    "        return im, recipe, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb6464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32e3185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function(batch):\n",
    "    imgs = []\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for img, text, label in batch:\n",
    "        \n",
    "        imgs.append(img)\n",
    "        texts.append(text)\n",
    "        labels.append(label)\n",
    "    \n",
    "    encoded_text_input = bert_tokenizer(texts, return_tensors='pt',padding='max_length', truncation=True).to(device)\n",
    "    imgs_feats = image_feature_extractor(imgs, return_tensors='pt')\n",
    "    labels = torch.tensor(labels)\n",
    "    return imgs_feats,encoded_text_input,labels\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceb1e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = MyDataset('train_sampled.csv','/freespace/local/sk2381/im2recipe-Pytorch/data/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575b7417",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_d = MyDataset('test.csv','/freespace/local/sk2381/im2recipe-Pytorch/data/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b61d8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_d, collate_fn=collate_function,batch_size=128, shuffle=False)\n",
    "test_dataloader = DataLoader(test_d, collate_fn=collate_function,batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea399e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transformer_model = MyModel(device, 768,768,2,1,2,512,0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60d7af27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51083c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(transformer_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3e8c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_full(epoch, dataloader, model, loss_fxn, optimizer, PATH):\n",
    "#     i=0\n",
    "#     k = 0\n",
    "#     with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "#         for img,recipe, y in tepoch:\n",
    "#             if i<=1000:\n",
    "#                 i+=1\n",
    "#                 k+=1\n",
    "#             else:\n",
    "#                 tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "#                 img = img.to(device)\n",
    "#                 recipe = recipe.to(device)\n",
    "\n",
    "#                 y = torch.reshape(y, (y.shape[0],1))\n",
    "#                 y = y.float()\n",
    "#                 y = y.to(device)\n",
    "\n",
    "#                 pred = model(img, recipe)\n",
    "#                 loss = loss_fn(pred, y)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 LOSS = loss.item()\n",
    "\n",
    "#                 tepoch.set_postfix(loss=loss.item())\n",
    "#                 if i%500 == 0:\n",
    "#                     torch.save({'epoch': epoch,'batch': k,'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict(),'loss': LOSS,}, PATH)\n",
    "#                 k+=1\n",
    "#         torch.save({'epoch': epoch,'batch': k,'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict(),'loss': LOSS,}, PATH + \"_final\")\n",
    "\n",
    "                \n",
    "def train_full(epoch, dataloader, model, loss_fxn, optimizer, PATH):\n",
    "    i=0\n",
    "    with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "        for img,recipe, y in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            img = img.to(device)\n",
    "            recipe = recipe.to(device)\n",
    "\n",
    "            y = torch.reshape(y, (y.shape[0],1))\n",
    "            y = y.float()\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(img, recipe)\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            LOSS = loss.item()\n",
    "\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "            if i%500 == 0:\n",
    "                torch.save({'epoch': epoch,'batch': i,'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict(),'loss': LOSS,}, PATH)\n",
    "            i+=1\n",
    "        torch.save({'epoch': epoch,'batch': i,'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict(),'loss': LOSS,}, PATH)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d6436d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('models_ins/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8761f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ca23193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                               | 0/2200 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  17%|██▎           | 372/2200 [59:18<5:26:34, 10.72s/batch, loss=0.692]Corrupt JPEG data: 22 extraneous bytes before marker 0xd9\n",
      "Epoch 0:  28%|███▍        | 620/2200 [1:42:16<3:32:49,  8.08s/batch, loss=0.744]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 0:  44%|█████▎      | 963/2200 [2:27:40<2:44:07,  7.96s/batch, loss=0.685]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 0:  75%|████████▎  | 1652/2200 [3:58:51<1:12:03,  7.89s/batch, loss=0.699]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 0:  96%|████████████▌| 2123/2200 [5:01:16<09:50,  7.66s/batch, loss=0.698]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 0: 100%|█████████████| 2200/2200 [5:11:27<00:00,  8.49s/batch, loss=0.726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "PATH = 'models_ins2/model.pt'\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_full(t,train_dataloader, transformer_model, loss_fn, optimizer, PATH)\n",
    "    \n",
    "    #test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a656ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
